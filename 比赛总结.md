## 1.报名参赛
研二以来一直想认真参加一回数据科学比赛，尤其是与图像相关的，但苦于精力有限且没有合适的比赛。4月初的时候，实验室同学波哥邀我一起参加阿里天池的FashionAI服饰
关键点检测比赛 ，见这个比赛与图像相关且还可以用到目标检测的相关技术（正好与我的研究课题相关），经过一番考虑后和波哥一起报了名。虽然比赛3月份就开始了，但
从我们报名到初赛截止还有3周左右的时间，还是有时间做的。
## 2.确定初步方案
报完名，在认真分析了赛题之后，我们明确了该题目是一个纯回归问题，给定blouse,skirt,outwear,dress,trousers这5种服饰类型图片预测出其对应的多个关键点位置，
评价指标DE（Detection Erorr）为距离归一化（针对每类）的平均关键点定位误差。我们初步的想法是先用目标检测技术检测出包围服饰关键点的bounding box，以缩小
后续回归模型的搜索空间，然后再用CNN做回归。我主要负责前面bounding box检测的部分，波哥则负责后面的关键点回归。
## 3.数据处理
主要是利用训练集给的各类服饰图片的关键点坐标生成相应的bounding box坐标(xmin,ymin,xmax,ymax)，然后做成COCO json格式，使得可以用Detectron框架训练目标
检测模型。在数据处理的时候，我犯了一个比较严重的错误，就是把COCO json的bounding box格式搞错了，官方格式是(x,y,w,h)，而我误以为是(xmin,ymin,xmax,ymax)
。导致用Faster R-CNN训练模型时经常异常中断，而且就算好不容易训练出来了，模型检测出的bounding box比实际目标的边界框也要大不少。尽管效果不理想，我们还是
在4月8号提交了第一次结果，测评结果是45%（第一名是百分之四点几）。通过用Eclipse断点调试了很久之后才发现问题的根源是上面的COCO json标签文件做的有问题。
另外，自己对OpenCV的图像数据格式也存在认识误区。OpenCV图像的坐标索引(x,y)表示的是(列索引，行索引），图像的shape顺序则是(height,width,channel)。哎，
只怪自己做数据的时候不够认真和细心呀，在那里消耗了大量的时间。
## 4.换用姿态估计方法
后面我的数据标签问题解决后，目标检测部分总算得到了预期的效果，并且我通过迁移学习（将在COCO上训练好的目标检测模型拿来训练自己的服饰数据集，需要在Detectron
代码的net.py文件中去掉模型最后的分类和回归层）将各类服饰的检测精度AP都提高到了85%以上（最好的超过了90%）。可是，波哥那边的关键点回归又效果不好了。波哥一
开始是直接自己写了个简单的三层CNN然后直接接了几层全连接层。后面波哥加了比赛的官方交流群，了解到那些排在排行榜前面做的比较好的选手大多用的是姿态估计的网络
（服饰关键点变化比较大，尤其是被模特穿在身上的时候，而人的姿态变化也是非常大的，这么一想用姿态估计的方法做效果好也就解释得通了），他就开始到网上找姿态估计
的网络来做，比如CPM，Hourglass等。后面用CPM训练得到的模型的预测效果果然有了比较大的提升（测评结果到了20%多），但CPM对某几类（blouse和outwear）的效果并
不是很好，模型训练了很久也提升不了。
## 5.换用Mask R-CNN的方法
波哥那边又没了起色后，我们曾一度打算弃疗了。后来，考虑到mask rcnn也能做关键点检测，他抱着试一试的心态让我用mask rcnn试试。我起初也不知从何下手，毕竟对
mask rcnn也不是太了解。但为了比赛，也只能硬着头皮去做了。其实，细读mask rcnn论文后才发现，mask rcnn检测关键点其实就是把关键点当作单个像素的mask，用
mask rcnn里面起实例分割作用的FCN头把关键点检测出来。大概弄清了原理后，实际训练mask rcnn并不难。首先，按照COCO json格式做好包含关键点坐标的标签文件，
然后参照Detectron里面关于keypoint检测的yaml配置文件进行配置，然后改一下跟关键点类别相关的代码就行了(主要在json_dataset.py和keypoint.py两个文件里）
。不过，这个时候我又犯了一个比较大的错误，在做数据标签的时候一些标签文件字典的category字段的class字段没有改成'person'，导致训练的时候程序没有完全识别
出所有训练数据集。还是后面在经常看到roidb被过滤掉很多之后才发现的。
## 6.初赛最后冲刺
把除了skirt之外的其它几类都换用mask rcnn预测后，结果一下提升了7个点，也帮助我们首次冲进了前100名，达到最好成绩97名。可惜好景不长，在保持了一轮前100名
后还是跌出了前100。后面我们虽然也通过增加迭代次数，把验证集也加进来一起训练等手段努力提升结果，但效果有限。不过，有点遗憾的是，最后初赛截止日期前一天
晚上，本来打算用全部数据多训练几个epoches的outwear中途因为我不够细心把数据集名字搞错了导致训练失败，最后一天没有用上。我们最后一次提交的成绩是11.2%,
第103名，差一点就能进复赛了，可惜呀！
## 7.幸运进入复赛
原本以为进不了复赛的，但由于初赛结果审核时前面有若干队伍未按要求提交，被取消了晋级复赛的资格，我们得以顺延进入复赛，也算是运气比较好。进入复赛后，波哥
选择使用夺得COCO 2017关键点检测冠军的级联金字塔网络模型CPN来做服饰关键点的检测（需要提供bounding box），一下将效果提升了一倍，定位误差降到了百分之五
点几，也帮我们首次冲进排行榜top20。然而，也是好景不长，等到众多比赛大佬发力，我们的排名不断在往下掉。虽然我们也想了一些办法企图保持名次，比如我负责的
检测这边通过数据增强和模型调参进一步提升检测精度，但由于已达到性能瓶颈，提升有限，对最终服饰关键点检测的效果提升也基本没有帮助；再比如波哥观察到检测出
的边界框偏紧凑，尝试让我把检测模型预测出的边界框人为扩大几个像素点，再拿去给CPN训练，效果也没有提升。后面我们也渐渐失去了干劲，没怎么做了。
## 8.复赛最后冲刺
差不多到距离复赛结束还有一周左右的时候，我们决心还是要最后努力一下，决定从改CPN的网络结构下手。先尝试在CPN中引入Squeeze-Excitation结构，但可能是由于
加的位置没选好，效果并没有提升，后来我在认真看了CPN原论文后并稍微对比了源码发现原论文中使用了ResNet-Inception结构而源码中貌似并没有看到Inception结构
，于是建议波哥把Inception结构加进源码，效果有了一点点提升。而后，复赛第二轮（时间1天）比赛主办方又更换了更大一批测试数据集，由于CPN推理比较耗时，只够
我们用加了Inception结构的模型重新推理一遍，我们已没有再修改调试模型的机会了。最终，我们复赛的成绩是第40名（newbie_team队，服饰关键点定位误差为4.41%）。至此，我们的
此处初战天池比赛的经历终于结束。


## 初次参加天池比赛的经验和教训：
1. 比赛初期的时候最好单独成队，这样可以多几次提交的机会，到后面再合并队伍。
2. 数据处理一定要仔细，在写程序的时候输出一些必要信息验证一下，如果数据处理出了问题，后面的环节都会受到影响。这次比赛我犯的两个比较大的错误都是在数据处理上。
3. 参加比赛的时候加一些比赛交流群或浏览比赛的技术讨论区，可以获得一些比赛方法的启发，甚至会有意想不到的收获。
4. 这次比赛代码管理和版本控制做的不是很好，git还是没有真正用起来，经常因为一些细节忘了改而出错，以后在开始建项目的时候就要有意识的用git进行版本控制。

